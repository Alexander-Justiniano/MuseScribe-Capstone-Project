{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHt2xZUSqIzV"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magenta/mt3/blob/main/mt3/colab/music_transcription_with_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Music Transcription with Transformers\n",
        "\n",
        "This notebook is an interactive demo of a few [music transcription models](g.co/magenta/mt3) created by Google's [Magenta](g.co/magenta) team.  You can upload audio and have one of our models automatically transcribe it.\n",
        "\n",
        "<img src=\"https://magenta.tensorflow.org/assets/transcription-with-transformers/architecture_diagram.png\" alt=\"Transformer-based transcription architecture\">\n",
        "\n",
        "The notebook supports two pre-trained models:\n",
        "1. the piano transcription model from [our ISMIR 2021 paper](https://archives.ismir.net/ismir2021/paper/000030.pdf)\n",
        "1. the multi-instrument transcription model from [our ICLR 2022 paper](https://openreview.net/pdf?id=iMSjopcOn0p)\n",
        "\n",
        "**Caveat**: neither model is trained on singing.  If you upload audio with vocals, you will likely get weird results.  Multi-instrument transcription is still not a completely-solved problem and so you may get weird results regardless.\n",
        "\n",
        "In any case, we hope you have fun transcribing!  Feel free to tweet any interesting output at [@GoogleMagenta](https://twitter.com/googlemagenta)...\n",
        "\n",
        "### Instructions for running:\n",
        "\n",
        "* Make sure to use a GPU runtime, click:  __Runtime >> Change Runtime Type >> GPU__\n",
        "* Press ▶️ on the left of each cell to execute the cell\n",
        "* In the __Load Model__ cell, choose either `ismir2021` for piano transcription or `mt3` for multi-instrument transcription\n",
        "* In the __Upload Audio__ cell, choose an MP3 or WAV file from your computer when prompted\n",
        "* Transcribe the audio using the __Transcribe Audio__ cell (it may take a few minutes depending on the length of the audio)\n",
        "\n",
        "---\n",
        "\n",
        "This notebook sends basic usage data to Google Analytics.  For more information, see [Google's privacy policy](https://policies.google.com/privacy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibSG_uu0QXgc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#@title Setup Environment\n",
        "#@markdown Install MT3 and its dependencies (may take a few minutes).\n",
        "\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth3 build-essential libasound2-dev libjack-dev\n",
        "\n",
        "# install mt3\n",
        "!git clone --branch=main https://github.com/magenta/mt3\n",
        "!mv mt3 mt3_tmp; mv mt3_tmp/* .; rm -r mt3_tmp\n",
        "!python3 -m pip install jax[cuda12] nest-asyncio pyfluidsynth==1.3.0 -e . -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "# copy checkpoints\n",
        "!gsutil -q -m cp -r gs://mt3/checkpoints .\n",
        "\n",
        "# copy soundfont (originally from https://sites.google.com/site/soundfonts4u)\n",
        "!gsutil -q -m cp gs://magentadata/soundfonts/SGM-v2.01-Sal-Guit-Bass-V1.3.sf2 .\n",
        "\n",
        "import json\n",
        "import IPython\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSMSWDxxWmTS"
      },
      "outputs": [],
      "source": [
        "# Import required libraries and modules for model inference, audio processing, and configuration.\n",
        "import functools\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import functools  # (Note: This import is repeated; one could remove duplicate imports.)\n",
        "import gin         # For configuration via gin files.\n",
        "import jax         # For JAX-based computations.\n",
        "import librosa     # For audio processing.\n",
        "import note_seq    # For working with note sequences.\n",
        "import seqio       # For sequence processing.\n",
        "import t5          # For T5 model support.\n",
        "import t5x         # For T5X model framework.\n",
        "\n",
        "# Import specific modules from the mt3 package used for transcription.\n",
        "from mt3 import metrics_utils\n",
        "from mt3 import models\n",
        "from mt3 import network\n",
        "from mt3 import note_sequences\n",
        "from mt3 import preprocessors\n",
        "from mt3 import spectrograms\n",
        "from mt3 import vocabularies\n",
        "\n",
        "# For file uploading in Colab\n",
        "from google.colab import files\n",
        "\n",
        "# Patch the event loop to support asynchronous operations in notebooks.\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Define global constants for the audio sample rate and the soundfont file path.\n",
        "SAMPLE_RATE = 16000\n",
        "SF2_PATH = 'SGM-v2.01-Sal-Guit-Bass-V1.3.sf2'\n",
        "\n",
        "def upload_audio(sample_rate):\n",
        "  \"\"\"\n",
        "  Prompt the user to upload audio files using Google Colab's file upload dialog.\n",
        "  Converts the first uploaded file's raw audio data into samples using note_seq.\n",
        "  \"\"\"\n",
        "  data = list(files.upload().values())\n",
        "  if len(data) > 1:\n",
        "    print('Multiple files uploaded; using only one.')\n",
        "  # Convert raw audio data to waveform samples.\n",
        "  return note_seq.audio_io.wav_data_to_samples_librosa(\n",
        "    data[0], sample_rate=sample_rate)\n",
        "\n",
        "class InferenceModel(object):\n",
        "  \"\"\"Wrapper of T5X model for music transcription.\n",
        "\n",
        "  This class handles model configuration, loading, and inference\n",
        "  for transcribing audio into musical note sequences.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, checkpoint_path, model_type='mt3'):\n",
        "    # Set model-specific constants.\n",
        "    if model_type == 'ismir2021':\n",
        "      # For the ismir2021 model, which transcribes piano only with note velocities.\n",
        "      num_velocity_bins = 127\n",
        "      self.encoding_spec = note_sequences.NoteEncodingSpec\n",
        "      self.inputs_length = 512\n",
        "    elif model_type == 'mt3':\n",
        "      # For the mt3 model, which transcribes multiple instruments without velocities.\n",
        "      num_velocity_bins = 1\n",
        "      self.encoding_spec = note_sequences.NoteEncodingWithTiesSpec\n",
        "      self.inputs_length = 256\n",
        "    else:\n",
        "      raise ValueError('unknown model_type: %s' % model_type)\n",
        "\n",
        "    # List the gin configuration files for the model.\n",
        "    gin_files = ['/content/mt3/gin/model.gin',\n",
        "                 f'/content/mt3/gin/{model_type}.gin']\n",
        "\n",
        "    # Set additional hyperparameters.\n",
        "    self.batch_size = 8\n",
        "    self.outputs_length = 1024\n",
        "    self.sequence_length = {'inputs': self.inputs_length,\n",
        "                            'targets': self.outputs_length}\n",
        "\n",
        "    # Create a partitioner for distributed model execution (here with 1 partition).\n",
        "    self.partitioner = t5x.partitioning.PjitPartitioner(\n",
        "        num_partitions=1)\n",
        "\n",
        "    # Build spectrogram configuration and codecs for converting note sequences.\n",
        "    self.spectrogram_config = spectrograms.SpectrogramConfig()\n",
        "    self.codec = vocabularies.build_codec(\n",
        "        vocab_config=vocabularies.VocabularyConfig(\n",
        "            num_velocity_bins=num_velocity_bins))\n",
        "    # Build vocabulary from the codec.\n",
        "    self.vocabulary = vocabularies.vocabulary_from_codec(self.codec)\n",
        "    # Define output features for the model (inputs are continuous; targets use the vocabulary).\n",
        "    self.output_features = {\n",
        "        'inputs': seqio.ContinuousFeature(dtype=tf.float32, rank=2),\n",
        "        'targets': seqio.Feature(vocabulary=self.vocabulary),\n",
        "    }\n",
        "\n",
        "    # Parse the gin configuration files.\n",
        "    self._parse_gin(gin_files)\n",
        "    # Load the model based on the configuration.\n",
        "    self.model = self._load_model()\n",
        "\n",
        "    # Restore the model's weights and optimizer state from a checkpoint.\n",
        "    self.restore_from_checkpoint(checkpoint_path)\n",
        "\n",
        "  @property\n",
        "  def input_shapes(self):\n",
        "    \"\"\"\n",
        "    Return the expected shapes of the input tensors for the model.\n",
        "    This is used for initializing the training state.\n",
        "    \"\"\"\n",
        "    return {\n",
        "          'encoder_input_tokens': (self.batch_size, self.inputs_length),\n",
        "          'decoder_input_tokens': (self.batch_size, self.outputs_length)\n",
        "    }\n",
        "\n",
        "  def _parse_gin(self, gin_files):\n",
        "    \"\"\"\n",
        "    Parse the provided gin configuration files to configure the model.\n",
        "    Gin files contain hyperparameters and other configuration settings.\n",
        "    \"\"\"\n",
        "    gin_bindings = [\n",
        "        'from __gin__ import dynamic_registration',\n",
        "        'from mt3 import vocabularies',\n",
        "        'VOCAB_CONFIG=@vocabularies.VocabularyConfig()',\n",
        "        'vocabularies.VocabularyConfig.num_velocity_bins=%NUM_VELOCITY_BINS'\n",
        "    ]\n",
        "    # Unlock the gin configuration to allow modifications, then parse files.\n",
        "    with gin.unlock_config():\n",
        "      gin.parse_config_files_and_bindings(\n",
        "          gin_files, gin_bindings, finalize_config=False)\n",
        "\n",
        "  def _load_model(self):\n",
        "    \"\"\"\n",
        "    Load a T5X model after gin configurations have been parsed.\n",
        "    Returns the model instance ready for inference.\n",
        "    \"\"\"\n",
        "    # Retrieve the model configuration from gin.\n",
        "    model_config = gin.get_configurable(network.T5Config)()\n",
        "    # Instantiate a Transformer module with the model configuration.\n",
        "    module = network.Transformer(config=model_config)\n",
        "    # Create the final Continuous Inputs Encoder-Decoder model,\n",
        "    # specifying the module, vocabularies, optimizer, and input depth.\n",
        "    return models.ContinuousInputsEncoderDecoderModel(\n",
        "        module=module,\n",
        "        input_vocabulary=self.output_features['inputs'].vocabulary,\n",
        "        output_vocabulary=self.output_features['targets'].vocabulary,\n",
        "        optimizer_def=t5x.adafactor.Adafactor(decay_rate=0.8, step_offset=0),\n",
        "        input_depth=spectrograms.input_depth(self.spectrogram_config))\n",
        "\n",
        "  def restore_from_checkpoint(self, checkpoint_path):\n",
        "    \"\"\"\n",
        "    Restore the model's state from the specified checkpoint.\n",
        "    This resets the prediction function and initializes training state.\n",
        "    \"\"\"\n",
        "    train_state_initializer = t5x.utils.TrainStateInitializer(\n",
        "      optimizer_def=self.model.optimizer_def,\n",
        "      init_fn=self.model.get_initial_variables,\n",
        "      input_shapes=self.input_shapes,\n",
        "      partitioner=self.partitioner)\n",
        "\n",
        "    restore_checkpoint_cfg = t5x.utils.RestoreCheckpointConfig(\n",
        "        path=checkpoint_path, mode='specific', dtype='float32')\n",
        "\n",
        "    train_state_axes = train_state_initializer.train_state_axes\n",
        "    self._predict_fn = self._get_predict_fn(train_state_axes)\n",
        "    self._train_state = train_state_initializer.from_checkpoint_or_scratch(\n",
        "        [restore_checkpoint_cfg], init_rng=jax.random.PRNGKey(0))\n",
        "\n",
        "  @functools.lru_cache()\n",
        "  def _get_predict_fn(self, train_state_axes):\n",
        "    \"\"\"\n",
        "    Generate a partitioned prediction function for decoding.\n",
        "    This function is cached to avoid re-partitioning on each call.\n",
        "    \"\"\"\n",
        "    def partial_predict_fn(params, batch, decode_rng):\n",
        "      return self.model.predict_batch_with_aux(\n",
        "          params, batch, decoder_params={'decode_rng': None})\n",
        "    return self.partitioner.partition(\n",
        "        partial_predict_fn,\n",
        "        in_axis_resources=(\n",
        "            train_state_axes.params,\n",
        "            t5x.partitioning.PartitionSpec('data',), None),\n",
        "        out_axis_resources=t5x.partitioning.PartitionSpec('data',)\n",
        "    )\n",
        "\n",
        "  def predict_tokens(self, batch, seed=0):\n",
        "    \"\"\"\n",
        "    Predict token sequences from a preprocessed batch.\n",
        "    Decodes the predicted tokens using the model's vocabulary.\n",
        "    \"\"\"\n",
        "    prediction, _ = self._predict_fn(\n",
        "        self._train_state.params, batch, jax.random.PRNGKey(seed))\n",
        "    return self.vocabulary.decode_tf(prediction).numpy()\n",
        "\n",
        "  def __call__(self, audio):\n",
        "    \"\"\"\n",
        "    Infer a note sequence from raw audio samples.\n",
        "\n",
        "    Args:\n",
        "      audio: 1-D numpy array of audio samples (16kHz) for a single example.\n",
        "\n",
        "    Returns:\n",
        "      A note_sequence object representing the transcribed audio.\n",
        "    \"\"\"\n",
        "    # Convert the audio into a dataset.\n",
        "    ds = self.audio_to_dataset(audio)\n",
        "    # Preprocess the dataset.\n",
        "    ds = self.preprocess(ds)\n",
        "    # Convert the dataset into batches and run predictions.\n",
        "    model_ds = self.model.FEATURE_CONVERTER_CLS(pack=False)(\n",
        "        ds, task_feature_lengths=self.sequence_length)\n",
        "    model_ds = model_ds.batch(self.batch_size)\n",
        "    # Predict tokens for each batch.\n",
        "    inferences = (tokens for batch in model_ds.as_numpy_iterator()\n",
        "                  for tokens in self.predict_tokens(batch))\n",
        "    predictions = []\n",
        "    # Pair each example with its predicted tokens.\n",
        "    for example, tokens in zip(ds.as_numpy_iterator(), inferences):\n",
        "      predictions.append(self.postprocess(tokens, example))\n",
        "    # Convert predicted events into a note sequence.\n",
        "    result = metrics_utils.event_predictions_to_ns(\n",
        "        predictions, codec=self.codec, encoding_spec=self.encoding_spec)\n",
        "    return result['est_ns']\n",
        "\n",
        "  def audio_to_dataset(self, audio):\n",
        "    \"\"\"\n",
        "    Create a TensorFlow Dataset of spectrograms from input audio.\n",
        "    \"\"\"\n",
        "    frames, frame_times = self._audio_to_frames(audio)\n",
        "    return tf.data.Dataset.from_tensors({\n",
        "        'inputs': frames,\n",
        "        'input_times': frame_times,\n",
        "    })\n",
        "\n",
        "  def _audio_to_frames(self, audio):\n",
        "    \"\"\"\n",
        "    Compute spectrogram frames from raw audio data.\n",
        "    Pads the audio if necessary and splits it into frames.\n",
        "    \"\"\"\n",
        "    frame_size = self.spectrogram_config.hop_width\n",
        "    # Calculate the padding needed to complete the final frame.\n",
        "    padding = [0, frame_size - len(audio) % frame_size]\n",
        "    audio = np.pad(audio, padding, mode='constant')\n",
        "    # Split the audio into frames.\n",
        "    frames = spectrograms.split_audio(audio, self.spectrogram_config)\n",
        "    num_frames = len(audio) // frame_size\n",
        "    # Compute the time for each frame.\n",
        "    times = np.arange(num_frames) / self.spectrogram_config.frames_per_second\n",
        "    return frames, times\n",
        "\n",
        "  def preprocess(self, ds):\n",
        "    \"\"\"\n",
        "    Apply a chain of preprocessing functions to the dataset.\n",
        "    This includes splitting tokens, adding dummy targets, and computing spectrograms.\n",
        "    \"\"\"\n",
        "    pp_chain = [\n",
        "        functools.partial(\n",
        "            t5.data.preprocessors.split_tokens_to_inputs_length,\n",
        "            sequence_length=self.sequence_length,\n",
        "            output_features=self.output_features,\n",
        "            feature_key='inputs',\n",
        "            additional_feature_keys=['input_times']),\n",
        "        preprocessors.add_dummy_targets,\n",
        "        functools.partial(\n",
        "            preprocessors.compute_spectrograms,\n",
        "            spectrogram_config=self.spectrogram_config)\n",
        "    ]\n",
        "    for pp in pp_chain:\n",
        "      ds = pp(ds)\n",
        "    return ds\n",
        "\n",
        "  def postprocess(self, tokens, example):\n",
        "    \"\"\"\n",
        "    Process predicted tokens and prepare the result for conversion to a note sequence.\n",
        "    Trims the end-of-sequence token and adjusts the start time.\n",
        "    \"\"\"\n",
        "    tokens = self._trim_eos(tokens)\n",
        "    start_time = example['input_times'][0]\n",
        "    # Adjust start time to the nearest symbolic token step.\n",
        "    start_time -= start_time % (1 / self.codec.steps_per_second)\n",
        "    return {\n",
        "        'est_tokens': tokens,\n",
        "        'start_time': start_time,\n",
        "        'raw_inputs': []  # Placeholder for raw inputs (not used here).\n",
        "    }\n",
        "\n",
        "  @staticmethod\n",
        "  def _trim_eos(tokens):\n",
        "    \"\"\"\n",
        "    Trim the end-of-sequence token from the token array.\n",
        "    \"\"\"\n",
        "    tokens = np.array(tokens, np.int32)\n",
        "    if vocabularies.DECODED_EOS_ID in tokens:\n",
        "      tokens = tokens[:np.argmax(tokens == vocabularies.DECODED_EOS_ID)]\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGQ-zpgy3raf"
      },
      "outputs": [],
      "source": [
        "#@title Load Model\n",
        "#@markdown The `ismir2021` model transcribes piano only, with note velocities.\n",
        "#@markdown The `mt3` model transcribes multiple simultaneous instruments,\n",
        "#@markdown but without velocities.\n",
        "\n",
        "MODEL = \"mt3\" #@param[\"ismir2021\", \"mt3\"]\n",
        "checkpoint_path = f'/content/checkpoints/{MODEL}/'\n",
        "inference_model = InferenceModel(checkpoint_path, MODEL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/mt3 -type f"
      ],
      "metadata": {
        "id": "KoY71qUK8KKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jwdj/EasyABC.git"
      ],
      "metadata": {
        "id": "hkxyui5z5o0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install flask and ngrok for http request and services\n",
        "# !pip install flask pyngrok\n",
        "!pip install fastapi uvicorn python-multipart pyngrok\n",
        "!rm ngrok\n",
        "!wget -q -O ngrok.zip https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -o ngrok.zip\n",
        "!./ngrok config add-authtoken 2tPc2RDNkkP31fLJon1HCWm0gjV_2V7dVSHND54Cc51Y3CWs9"
      ],
      "metadata": {
        "id": "cIIvl2MkAnLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Python App Code (FastAPI with Improved Security, No Shutdown Endpoint)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import uuid\n",
        "import subprocess\n",
        "import uvicorn\n",
        "import threading\n",
        "import nest_asyncio\n",
        "import logging\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException, Request\n",
        "from fastapi.responses import HTMLResponse, JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok, conf\n",
        "from note_seq import notebook_utils, sequence_proto_to_midi_file\n",
        "import tensorflow as tf\n",
        "from note_seq.audio_io import wav_data_to_samples_librosa\n",
        "from werkzeug.utils import secure_filename\n",
        "\n",
        "# Apply nest_asyncio so that uvicorn can run in a notebook environment\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ---------------------------\n",
        "# Logging Configuration\n",
        "# ---------------------------\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"FastAPI-Security\")\n",
        "\n",
        "# ---------------------------\n",
        "# ngrok Configuration\n",
        "# ---------------------------\n",
        "conf.get_default().ngrok_path = \"ngrok\"\n",
        "NGROK_TOKEN = \"2tPc2RDNkkP31fLJon1HCWm0gjV_2V7dVSHND54Cc51Y3CWs9\"  # Replace with your actual token\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "# Start ngrok tunnel on port 5000 with custom hostname\n",
        "public_url = ngrok.connect(5000, bind_tls=True, hostname=\"secure-darling-minnow.ngrok-free.app\").public_url\n",
        "logger.info(f\" * ngrok tunnel: {public_url}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Global Values\n",
        "# ---------------------------\n",
        "SAMPLE_RATE = 16000\n",
        "SF2_PATH    = 'SGM-v2.01-Sal-Guit-Bass-V1.3.sf2'\n",
        "\n",
        "# Allowed origins for CORS, including localhost\n",
        "ALLOWED_ORIGINS = [\n",
        "    \"http://localhost\",\n",
        "    \"http://localhost:5000\",\n",
        "    \"http://localhost:8080\",\n",
        "    public_url  # For testing via ngrok\n",
        "]\n",
        "\n",
        "# ---------------------------\n",
        "# Model Loading Placeholder\n",
        "# ---------------------------\n",
        "def load_model():\n",
        "    global inference_model\n",
        "    if inference_model is None:\n",
        "        raise ValueError(\"Model not loaded. Please run the 'Load Model' cell first.\")\n",
        "    return inference_model\n",
        "\n",
        "# ---------------------------\n",
        "# Audio Transcription Function\n",
        "# ---------------------------\n",
        "def transcribe_audio(file_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Transcribe audio to MIDI and return a dictionary.\n",
        "    This function loads the audio file, processes it using the model to generate a MIDI representation,\n",
        "    writes the resulting MIDI to /tmp folder, and then returns the MIDI data as a dictionary.\n",
        "    \"\"\"\n",
        "    print(file_path)\n",
        "\n",
        "    # Open the specified file in binary mode and read its content.\n",
        "    with open(file_path, 'rb') as f:\n",
        "        audio_data = f.read()\n",
        "\n",
        "    # Convert the raw audio data to a waveform using a helper function from note_seq.\n",
        "    # The SAMPLE_RATE constant defines the number of samples per second.\n",
        "    audio = wav_data_to_samples_librosa(audio_data, sample_rate=SAMPLE_RATE)\n",
        "\n",
        "    # Load the transcription model. This function is expected to return a pre-loaded model.\n",
        "    model = load_model()\n",
        "\n",
        "    # Process the audio data using the loaded model to obtain an estimated note sequence.\n",
        "    est_ns = model(audio)\n",
        "\n",
        "    # Define the path where the generated MIDI file will be stored.\n",
        "    # midi_file_path = '/tmp/transcribed.mid'\n",
        "\n",
        "    # Convert the estimated note sequence (est_ns) to a MIDI file and save it to the specified path.\n",
        "    sequence_proto_to_midi_file(est_ns, file_path)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Client MIDI File to ABC Conversion Function\n",
        "# ---------------------------\n",
        "def midi_to_abcnotation(midi_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Convert the given MIDI to ABC notation using EasyABC's midi2abc.py script.\n",
        "    Returns a dictionary with key 'abc_notation'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conversion_command = [sys.executable, \"/content/EasyABC/midi2abc.py\", midi_path]\n",
        "        conversion_result = subprocess.run(\n",
        "            conversion_command,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "        return {\"abc_notation\": conversion_result.stdout}\n",
        "    except subprocess.CalledProcessError as e:\n",
        "      logger.error(\"MIDI-to-ABC conversion failed.\", exc_info=True)\n",
        "      # raise an HTTPException so FastAPI returns a 500\n",
        "      raise HTTPException(\n",
        "          status_code=500,\n",
        "          detail=f\"Failed to convert MIDI to ABC: {e.stderr.strip()}\"\n",
        "      )\n",
        "\n",
        "# ---------------------------\n",
        "# FastAPI Application Setup\n",
        "# ---------------------------\n",
        "app = FastAPI()\n",
        "\n",
        "# Set up CORS with restricted origins\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=ALLOWED_ORIGINS,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def home() -> HTMLResponse:\n",
        "    \"\"\"\n",
        "    Home endpoint that returns a simple HTML page.\n",
        "    \"\"\"\n",
        "    html_content = (\n",
        "        \"<h1>Music Transcription API - Midi -> ABC Notation</h1>\"\n",
        "        \"<p>Upload audio files to /upload</p>\"\n",
        "    )\n",
        "    return HTMLResponse(content=html_content)\n",
        "\n",
        "@app.get(\"/test-data\")\n",
        "async def test_data():\n",
        "    \"\"\"\n",
        "    Returns transcribed MIDI data in ABC notation as JSON.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        midi_data = midi_to_abcnotation()\n",
        "        return JSONResponse(content={\"transcription\": midi_data})\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Error in /test-data endpoint.\")\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "@app.post(\"/upload\")\n",
        "async def upload_audio(file: UploadFile = File(...)):\n",
        "    \"\"\"\n",
        "    Accepts an uploaded audio file (mp3 or wav), processes it to generate MIDI,\n",
        "    converts the MIDI to ABC notation, removes the temporary MIDI file,\n",
        "    and returns the ABC notation as JSON.\n",
        "    \"\"\"\n",
        "    # Validate the file extension to ensure it's an MP3 or WAV.\n",
        "    if not (file.filename.lower().endswith(('.mp3', '.wav', '.mid', '.midi'))):\n",
        "        raise HTTPException(status_code=400, detail=\"Invalid file type\")\n",
        "\n",
        "    # Extract original extension and generate a unique filename\n",
        "    ext = os.path.splitext(file.filename)[1]\n",
        "    unique_name = f\"{uuid.uuid4().hex}{ext}\"\n",
        "\n",
        "    # Create full path in temp dir\n",
        "    file_path = os.path.join(\"/tmp\", unique_name)\n",
        "\n",
        "    # Open the destination file in binary write mode and save the uploaded content.\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        # Asynchronously read the file content.\n",
        "        content = await file.read()\n",
        "        # Write the file content to disk.\n",
        "        f.write(content)\n",
        "\n",
        "    # 1) if audio -> transcribe\n",
        "    if file.filename.lower().endswith(('.mp3', '.wav')):\n",
        "        try:\n",
        "            transcribe_audio(file_path)\n",
        "        except Exception as e:\n",
        "            logger.error(\"Audio transcription failed.\", exc_info=True)\n",
        "            raise HTTPException(\n",
        "                status_code=500,\n",
        "                detail=f\"Audio transcription error: {str(e)}\"\n",
        "            )\n",
        "\n",
        "    # 2) convert MIDI -> ABC (raises HTTPException on fail)\n",
        "    midi_data = midi_to_abcnotation(file_path)\n",
        "\n",
        "    # 3) cleanup temp file\n",
        "    try:\n",
        "        os.remove(file_path)\n",
        "    except OSError:\n",
        "        logger.warning(\"Could not delete temp file %s\", file_path)\n",
        "\n",
        "    return JSONResponse(content={\"transcription\": midi_data})\n",
        "\n",
        "# ---------------------------\n",
        "# Run the Server Directly\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=5000, log_level=\"info\")\n"
      ],
      "metadata": {
        "id": "04aMcYNwVSIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Kill Port to restart server\n",
        "\n",
        "# List current ngrok tunnels\n",
        "# !curl http://127.0.0.1:4040/api/tunnels\n",
        "\n",
        "\n",
        "# Kill the uvicorn process\n",
        "# !kill 51096\n",
        "\n",
        "# Check which processes are listening on port 5000\n",
        "!netstat -tulpn | grep 5000"
      ],
      "metadata": {
        "id": "wgfpsCToVjUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2s3JAvBAgFB"
      },
      "outputs": [],
      "source": [
        "#@title Upload Audio\n",
        "\n",
        "# audio = upload_audio(sample_rate=SAMPLE_RATE)\n",
        "\n",
        "# note_seq.notebook_utils.colab_play(audio, sample_rate=SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSKNjUYYv1kV"
      },
      "outputs": [],
      "source": [
        "#@title Transcribe Audio\n",
        "#@markdown This may take a few minutes depending on the length of the audio file\n",
        "#@markdown you uploaded.\n",
        "\n",
        "# est_ns = inference_model(audio)\n",
        "\n",
        "# note_seq.play_sequence(est_ns, synth=note_seq.fluidsynth,\n",
        "#                        sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
        "# note_seq.plot_sequence(est_ns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DiCjtDpyUMh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download MIDI Transcription\n",
        "\n",
        "# note_seq.sequence_proto_to_midi_file(est_ns, '/tmp/transcribed.mid')\n",
        "# files.download('/tmp/transcribed.mid')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Music_Transcription_with_Transformers.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}